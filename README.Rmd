---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# faLearn

<!-- badges: start -->
[![R-CMD-check](https://github.com/agerada/faLearn/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/agerada/faLearn/actions/workflows/R-CMD-check.yaml)
<!-- badges: end -->

## Introduction

`faLearn` provides utilities to transform whole-genome sequence data into
feature representations suitable for machine learning, with a focus on k-mer
based features and XGBoost-compatible input (libsvm). The package is intended
for workflows that start from raw or assembled genome FASTA/FNA files and end
with model-ready data.

This repository was migrated from a prior package; MIC-specific functions have
been removed â€” the current scope is machine learning with genome sequence data.

## Main features

- Convert individual genomes (FASTA/FNA) into k-mer counts.
- Export k-mer counts in XGBoost-friendly libsvm format (one .txt per genome).
- Fast k-mer counting implemented in C++ (Rcpp) for performance.
- Helpers to process directories of genomes in parallel (via future.apply) and
  to split/combine libsvm files for training/validation workflows.
- Memory-efficient XGBoost cross-validation using [xgb.cv.lowmem](man/xgb.cv.lowmem.Rd).

## Installation

Install from GitHub (development version):

```r
# install.packages("remotes")
remotes::install_github("agerada/faLearn")
```

## Quick examples

[`libsvm`]("https://xgboost.readthedocs.io/en/stable/tutorials/input_format.html") is the preferred
input format for XGBoost. Data are represented as a sparce matrix in a special text format.

To convert a directory of genomes (.fna or .fasta) directly to libsvm files (one line per
genome):

```{r}
library(faLearn)
data("example_genomes", package = "faLearn")

# use the first genome from the packaged example_genomes
genome_ds <- example_genomes$genomes[[1]]
tmp_out <- file.path(tempdir(), "kmers")
unlink(tmp_out, recursive = TRUE)
dir.create(tmp_out, recursive = TRUE, showWarnings = FALSE)
target_file <- file.path(normalizePath(tmp_out), paste0(names(example_genomes$genomes)[1], ".txt"))
future::plan(future::sequential) # or multisession, etc.
progressr::with_progress({
  genome_to_libsvm(as.character(genome_ds),
                   target_file,
                   k = 5)
})

list.files(tmp_out)
readLines(list.files(tmp_out, full.names = TRUE)[1])
```

Count k-mers for a single sequence string using the fast C++ implementation:

```{r}
kmers("ATCGATCGA", k = 3)
```

Train a small XGBoost model (cross-validated) using k-mer features and
`xgb.cv.lowmem`. This example simulates a numeric phenotype (e.g., an MIC
value) and demonstrates how to build a feature matrix from multiple genome
files using `kmers()` (with `anchor = TRUE` so feature lengths align).

```{r}
data("example_genomes", package = "faLearn")

genomes_list <- example_genomes$genomes

# create kmer libsvm features saved to disk
tmp_out <- file.path(tempdir(), "kmers_all")
unlink(tmp_out, recursive = TRUE)
dir.create(tmp_out, recursive = TRUE, showWarnings = FALSE)
future::plan(future::sequential) # or multisession, etc.
progressr::with_progress({
  # iterate over genomes by index so we can use the genome id (not contig names)
  for (i in seq_along(genomes_list)) {
    g <- genomes_list[[i]]
    gid <- names(genomes_list)[i]
    target_file <- file.path(normalizePath(tmp_out), paste0(gid, ".txt"))
    genome_to_libsvm(as.character(g), target_file, k = 5)
  }
})

# build k-mer features in memory (used below to create feature matrix)
features_list <- lapply(genomes_list, function(ds) {
  kmers(as.character(ds), k = 5, simplify = TRUE, anchor = TRUE, canonical = TRUE)
})

n_genomes <- length(genomes_list)
feature_mat <- do.call(rbind, lapply(features_list, as.numeric))
rownames(feature_mat) <- names(genomes_list)

# labels are stored in the packaged object (log2 scale)
labels <- example_genomes$labels[seq_len(n_genomes)]

# create xgboost DMatrix and run low-memory CV
dtrain <- xgboost::xgb.DMatrix(data = feature_mat, label = labels)

cv <- xgb.cv.lowmem(params = list(objective = "reg:squarederror", eval_metric = "rmse", nthread = 1),
                    data = dtrain,
                    nrounds = 20,
                    nfold = 4,
                    prediction = TRUE,
                    nthread = 1)

head(cv$evaluation_log)
```

## Notes and next steps

- The package focuses on feature generation. Model training (XGBoost) is
  expected to be performed outside the package using standard tools.
- If you previously used MIC functions, consult the function reference to map
  older workflows to the new `faLearn` utilities.

## Contact

Report issues or feature requests on the GitHub repository: 
https://github.com/agerada/faLearn
